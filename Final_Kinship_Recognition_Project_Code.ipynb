{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QDqx72BXjZL"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PbSdXO1kXp0j"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install keras_vggface\n",
    "!pip install keras_applications\n",
    "#!pip install numpy==1.19.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tGmI9--fXq6U"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Average, Reshape, Lambda, Add, Conv2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras.models import load_model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hGbDMYU9XwdY"
   },
   "outputs": [],
   "source": [
    "# Modify paths as per your method of saving them\n",
    "train_file_path = \"/gdrive/MyDrive/Kinship Recognition Starter/train_ds.csv\"\n",
    "train_folders_path = \"/gdrive/MyDrive/Kinship Recognition Starter/train/train-faces/\"\n",
    "# All images belonging to families F09** will be used to create the validation set while training the model\n",
    "# For final submission, you can add these to the training data as well\n",
    "val_famillies = \"F09\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0Ob1y_ZKXw2B"
   },
   "outputs": [],
   "source": [
    "all_images = glob(train_folders_path + \"*/*/*.jpg\")\n",
    "\n",
    "train_images = [x for x in all_images if val_famillies not in x]\n",
    "val_images = [x for x in all_images if val_famillies in x]\n",
    "\n",
    "train_person_to_images_map = defaultdict(list)\n",
    "\n",
    "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n",
    "\n",
    "for x in train_images:\n",
    "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "val_person_to_images_map = defaultdict(list)\n",
    "\n",
    "for x in val_images:\n",
    "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "J3xSkH6vXy_u"
   },
   "outputs": [],
   "source": [
    "relationships = pd.read_csv(train_file_path)\n",
    "relationships = list(zip(relationships.p1.values, relationships.p2.values, relationships.relationship.values))\n",
    "relationships = [(x[0],x[1],x[2]) for x in relationships if x[0][:10] in ppl and x[1][:10] in ppl]\n",
    "\n",
    "train = [x for x in relationships if val_famillies not in x[0]]\n",
    "val = [x for x in relationships if val_famillies in x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vCCBjCp7X1aT"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "#For VGGFace we need 224 x 224\n",
    "def read_img(path):\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    img = np.array(img).astype(np.float)\n",
    "    return preprocess_input(img, version=2)\n",
    "#For FaceNet we need 160 x 160\n",
    "def read_img2(path):\n",
    "    img = image.load_img(path, target_size=(160, 160))\n",
    "    img = np.array(img).astype(np.float)\n",
    "    return preprocess_input(img, version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LLBpey2-X4SD"
   },
   "outputs": [],
   "source": [
    "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size)\n",
    "        \n",
    "        # All the samples are taken from train_ds.csv, labels are in the labels column\n",
    "        labels = []\n",
    "        for tup in batch_tuples:\n",
    "          labels.append(tup[2])\n",
    "        \n",
    "        #creating 4 inputs - 2 for VGG Face and 2 for FaceNet. Since FaceNet format is 160 x 160 we must use read_img2. \n",
    "        X1 = [x[0] for x in batch_tuples]\n",
    "        X1 = np.array([read_img(train_folders_path + x) for x in X1])\n",
    "        X1_2 = np.array([read_img2(train_folders_path + x) for x in X1])\n",
    "\n",
    "        X2 = [x[1] for x in batch_tuples]\n",
    "        X2 = np.array([read_img(train_folders_path + x) for x in X2])\n",
    "        X2_2 = np.array([read_img2(train_folders_path + x) for x in X2])\n",
    "\n",
    "        yield [X1_1, X2_1, X1_2, X2_2], np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6bSwdDW8YJab"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution \n",
    "disable_eager_execution()\n",
    "\n",
    "def baseline_model():\n",
    "    \n",
    "    # 4 inputs - 2 for VGG Face and 2 for FaceNet\n",
    "    \n",
    "    #VGGFace input\n",
    "    input_1 = Input(shape=(224, 224, 3))\n",
    "    input_2 = Input(shape=(224, 224, 3))\n",
    "    #FaceNet input\n",
    "    input_3 = Input(shape=(160, 160, 3))\n",
    "    input_4 = Input(shape=(160, 160, 3)) \n",
    "\n",
    "    #loading VGGFace and FaceNet models\n",
    "    base_model = VGGFace(model='resnet50', include_top=False)\n",
    "    base_model2 = load_model('/gdrive/MyDrive/Kinship Recognition Starter/facenet_keras.h5')\n",
    "    \n",
    "    for x in base_model.layers[:-3]:\n",
    "        x.trainable = True\n",
    "\n",
    "    for x in base_model2.layers[:-3]:\n",
    "        x.trainable = True\n",
    "\n",
    "    #creating embedding based on both models\n",
    "    x1 = base_model(input_1)\n",
    "    x2 = base_model(input_2)\n",
    "    x3 = Reshape((1, 1 ,128))(base_model2(input_3))\n",
    "    x4 = Reshape((1, 1 ,128))(base_model2(input_4))\n",
    "\n",
    "    #Applying Various Operations\n",
    "    x5 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1), GlobalMaxPool2D()(x3), GlobalAvgPool2D()(x3)])\n",
    "    x6 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2), GlobalMaxPool2D()(x4), GlobalAvgPool2D()(x4)])\n",
    "    x7 = Subtract()([x5, x6])\n",
    "    x8 = Add()([x5, x6])\n",
    "    x9 = Multiply()([x5, x6])\n",
    "    \n",
    "    #Concatenating\n",
    "    x = Concatenate(axis=-1)([x7, x8, x9])\n",
    "    \n",
    "    #Dense Layers + Dropout Regularization\n",
    "    x = Dense(250, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(100, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_1, input_2, input_3, input_4], out)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adamax(0.0001))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TT11LMpkYT3O"
   },
   "outputs": [],
   "source": [
    "file_path = \"/gdrive/MyDrive/vgg_face.h5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "\n",
    "reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_on_plateau]\n",
    "\n",
    "model = baseline_model()\n",
    "model.fit(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=False,\n",
    "                validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=25, verbose=1, steps_per_epoch=100,\n",
    "                workers=1, validation_steps=50, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoMV2uBYYoyU"
   },
   "outputs": [],
   "source": [
    "# Modify paths as per your need\n",
    "test_path = \"/gdrive/MyDrive/Kinship Recognition Starter/test/\"\n",
    "\n",
    "model = baseline_model()\n",
    "model.load_weights(\"/gdrive/MyDrive/vgg_face.h5\")\n",
    "\n",
    "submission = pd.read_csv('/gdrive/MyDrive/Kinship Recognition Starter/test_ds.csv')\n",
    "predictions = []\n",
    "\n",
    "for i in range(0, len(submission.p1.values), 32):\n",
    "    X1 = submission.p1.values[i:i+32]\n",
    "    X1 = np.array([read_img(test_path + x) for x in X1])\n",
    "\n",
    "    X2 = submission.p2.values[i:i+32]\n",
    "    X2 = np.array([read_img(test_path + x) for x in X2])\n",
    "\n",
    "    pred = model.predict([X1, X2]).ravel().tolist()\n",
    "    predictions += pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhhCkgY_Yq-q"
   },
   "outputs": [],
   "source": [
    "d = {'index': np.arange(0, 3000, 1), 'label':predictions}\n",
    "submissionfile = pd.DataFrame(data=d)\n",
    "submissionfile = submissionfile.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7_JzEkGYtdR"
   },
   "outputs": [],
   "source": [
    "submissionfile.astype(\"int64\").to_csv(\"/gdrive/MyDrive/Kinship Recognition Starter/sv2637.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Final_Kinship_Recognition_Project_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
